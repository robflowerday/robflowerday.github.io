<!DOCTYPE html>
<head></head>
<body><!DOCTYPE html>
<head>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <ul class="top_nav">
        <li><a href="machinelearningwithpython.html">Machine Learning with Python</a></li>
        <li><a href="cv.html">CV</a></li>
    </ul>
    
    <h1>K-Neareset Neighbors</h1>
    <h2>What is KNN</h2>
    <p>K-Nearest Neighbors or KNN is a supervised learning model, which means it learns from <br>
        labeled data, it is Classification model rather than a regression model meaning the <br>
        labeled data is discrete. e.g. using email content and sender to detect whether an <br>
        email is spam or not.<br>
        <br>
        We'll go through the intuition of how a knn classifier works, then we will code one up.
    </p>

    <h2>Importing the Data</h2>
    <p>In this lesson we will use a dataset called the iris dataset which can be found in<br> 
        the sci-kit learn API. This is a very popular library for Machine Learning, as you<br> 
        progress through the course you will see why.<br>
        <br>
        To import the dataset use the following commands<br>
        <br>
        from sklearn import datasets<br>
        iris = datasets.load_iris()<br>
        <br>
        This will give you a dictionary, the dictionary holds several keys, the names of which<br>
        can be accessed with the command<br>
        <br>
        iris.keys()<br>
        <br>
        They consist of:<br>
            data<br>
            target<br>
            target_names<br>
            DESCR<br>
            feature_names<br>
            filename<br>
            <br>
        data: gives the feature data that we will be used to train our model and make<br>
            predictions of the target variable, in this case the observations of <br>
            sepal width, sepal length, petal width and petal length of 3 species of plants<br>
        target: the actual values (labels) of the target variables, in this case species of<br>
            plant<br>
        target_names: the different possible values that the target variable can take<br>
        DESCR: A description of the dataset<br>
        feature_names: the names of the columns (features) in the dataset, in this case - <br>
            sepal width, sepal length, petal width and petal length<br>
        filename: the path where the csv file containing the data is stored on your machine<br>
    </p>

    <h2>How KNN works</h2>
    <p>A KNN classifier predicts the label of a given datapoint by finding the 'k' nearst<br>
        labeled data points, where k is decided by the programmer, and labeling the datapoint <br>
        with the most common label of the k points. Essentially giving each of the k neighbors <br>
        a vote, and going with the majority.</p>

    <h2>Intuition through Visualisation</h2>
    <p>KNN is easiest visualised when we only work with 2 feauture variables (so that we can <br>
        visualise it in 2d on a graph). as well as using a scatter plot where we can colour <br>
        the data points by the target variable.<br>
        <br>
        Below is a visualisation of a scatterplot showing red data points, indicating one <br>
        value the target variable could take and blue data points indicating another value the<br> 
        target variable could take. The value of the green points will be decided bt the <br>
        K-Nearest Neighbors model. The position on the scatter graph of each point is <br>
        determined by 2 vfeatures of the data point (feature data).<br>
        <br>
        I have randomised all data points, but the x and y values represent two seperate <br>
        feature variables and the colours red and blue represent the target variable<br>
    </p>

    <img src="Images/knnvisual.png" alt="KNN Visualisation Graph">

    <p>With the K-Nearest Neighbors models, we have to choose one parameter. This parameter <br>
        can be chosen arbitrarily. Here will choose k to be three.<br>
        <br>
        What this means is that when predicting a label for a new data point, it will pick the <br>
        most common label of the three closest data points.<br>
        <br>
        We can see this in the updated graph below.<br>
    </p>

    <img src="Images/knnvisualmod.png" alt="KNN Visualisation Graph k = 3">

    <p>This graph shows the 3 nearest neighbors of each nre data point (shown in green).<br>
        Two of the points nearest point 1 are blue and 1 is red, this means our KNN model with <br>
        k = 3 will predict this point to have the label blue.<br>
        The three nearest neigbors to point number two have labels blue, blue, red. There are <br>
        more blues than reds and so point number two would be labeled blue by our model.<br>
        Finally, point number 3 has 1 blue point and 2 red points as its 3 closest data points, <br>
        and so our model would label it as red.<br>
    </p>

    <h2>Creating a Classifier</h2>
    <p>Machine learning models learn from and predict data. They do this executing algorithms <br>
        on data given to the model and storing the information learned. We call executing <br>
        these algorithms, training the model. This training in most models is executed by <br>
        calling the .fit() method on the model. The model then holds the stored data learned <br>
        from the model.<br>
        <br>
        After this we can use the .predict() method on most models to predict a new label  <br>
        for any given set of data as the model has been trained. <br>
    </p>

    <p>To create a KNN model in python we need to import the data in the same way as <br>
        previously explained: <br>
    </p>
    
    <img src="Images/importdataset.png" alt="from sklearn import datasets">

    <p>We then need to import the clasification model from the sci-kit-learn library
    </p>
    
    <img src="Images/importknn.png" alt="from sklearn.neighbors import KNeighborsClassifier">
    
    <p>We can now instantiate a K Neighbors Classifier using an arbitrary value of 3 neighbors.
    </p>
    
    <img src="Images/createmodel.png" alt="model = KNeighborsClassifier(n_neighbors=3)">
   
    <p>Then we can fit the model to the data passing the feature data as the <br>
        first parameter and the target data as the second argument to the fit method.
    </p>
    
    <img src="Images/fitmodel.png" alt="model.fit(iris.data, iris.target)">
   
    <p>We can then create some new data points using numpy.
    </p>
    
    <img src="Images/numpynewpoints.png" alt="model.fit(import numpy as np)">
   
    <p>Then we can predict the target values for the feature data we have just created.
    </p>
    
    <img src="Images/targetvalues.png" alt="predictions = model.predict(new_points)">
    
    <a href="typesofmachinelearning.html">Previous: Different Types of Machine Learning</a>
    <a href="trainingdata.html">Up Next: Train Test Split</a>
</body>
