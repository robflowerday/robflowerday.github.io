<!DOCTYPE html>
<head></head>
<body>
    <h1>K-Neareset Neighbors</h1>
    <h2>What is KNN</h2>
    <p>K-Nearest Neighbors or KNN is a Classification model, which means it learns from <br>
        labeled data, e.g. using email content and sender to detect whether an email is <br>
        spam or not.<br>
        <br>
        We'll go through the intuition of how a knn classifier works, then we will code one up.
    </p>

    <h2>Importing the Data</h2>
    <p>In this lesson we will use a dataset called the iris dataset which can be found in<br> 
        the sci-kit learn API. This is a very popular library for Machine Learning, as you<br> 
        progress through the course you will see why.<br>
        <br>
        To import the dataset use the following commands<br>
        <br>
        from sklearn import datasets<br>
        iris = datasets.load_iris()<br>
        <br>
        This will give you a dictionary, the dictionary holds several keys, the names of which<br>
        can be accessed with the command<br>
        <br>
        iris.keys()<br>
        <br>
        They consist of:<br>
            data<br>
            target<br>
            target_names<br>
            DESCR<br>
            feature_names<br>
            filename<br>
            <br>
        data: gives the feature data that we will be used to train our model and make<br>
            predictions of the target variable, in this case the observations of <br>
            sepal width, sepal length, petal width and petal length of 3 species of plants<br>
        target: the actual values (labels) of the target variables, in this case species of<br>
            plant<br>
        target_names: the different possible values that the target variable can take<br>
        DESCR: A description of the dataset<br>
        feature_names: the names of the columns (features) in the dataset, in this case - <br>
            sepal width, sepal length, petal width and petal length<br>
        filename: the path where the csv file containing the data is stored on your machine<br>
    </p>

    <h2>How KNN works</h2>
    <p>A KNN classifier predicts the label of a given datapoint by finding the 'k' nearst<br>
        labeled data points, where k is decided by the programmer, and labeling the datapoint <br>
        with the most common label of the k points. Essentially giving each of the k neighbors <br>
        a vote, and going with the majority.</p>

    <h2>Visualising KNN</h2>
    <p>KNN is easiest visualised when we only work with 2 feauture variables (so that we can <br>
        visualise it in 2d on a graph). as well as using a scatter plot where we can colour <br>
        the data points by the target variable.<br>
        <br>
        Below is a visualisation of a scatterplot showing red data points, indicating one <br>
        value the target variable could take and blue data points indicating another value the<br> 
        target variable could take. The value of the green points will be decided bt the <br>
        K-Nearest Neighbors model. The position on the scatter graph of each point is <br>
        determined by 2 vfeatures of the data point (feature data).<br>
        <br>
        I have randomised all data points, but the x and y values represent two seperate <br>
        feature variables and the colours red and blue represent the target variable<br>
    </p>

    <img src="knnvisual.png" alt="KNN Visualisation Graph">

    <p>With the K-Nearest Neighbors models, we have to choose one parameter. This parameter <br>
        can be chosen arbitrarily. Here will choose k to be three.<br>
        <br>
        What this means is that when predicting a label for a new data point, it will pick the <br>
        most common label of the three closest data points.<br>
        <br>
        We can see this in the updated graph below.<br>
    </p>

    <img src="knnvisualmod.png" alt="KNN Visualisation Graph k = 3">

    <p>This graph shows the 3 nearest neighbors of each nre data point (shown in green).<br>
        Two of the points nearest point 1 are blue and 1 is red, this means our KNN model with <br>
        k = 3 will predict this point to have the label blue.<br>
        The three nearest neigbors to point number two have labels blue, blue, red. There are <br>
        more blues than reds and so point number two would be labeled blue by our model.<br>
        Finally, point number 3 has 1 blue point and 2 red points as its 3 closest data points, <br>
        and so our model would label it as red.<br>
    </p>
</body>
