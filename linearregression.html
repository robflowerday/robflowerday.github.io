<!DOCTYPE html>
<head></head>
<body><!DOCTYPE html>
<head>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <ul class="top_nav">
        <li><a href="machinelearningwithpython.html">Machine Learning with Python</a></li>
        <li><a href="cv.html">CV</a></li>
    </ul>
    
    <h1>Linear Regression</h1>
    <p> When talking about Machine Learning, Linear Regression is a supervised learning <br>
        algorithm like our KNN model, unlike our KNN model however it is a regression model <br>
        rather than a clasification model. This means that is predicts label as continous <br>
        values rather than discrete values. Linear Regression models are used to attempt to 
        describe a linear relationship between two factors, based on a correlation between 
        the two factors.<br>
        <br>
        A correlation between to factos is a measure that describe whether two factors are 
        related to one another. If as the value of one variable increases, the other also 
        increases, we have a positive correlation, if as the value of one varible increases, 
        the other decreases, there is a negative correlation and if the value of one variable 
        going up or down, has no bearing on the other variable going up or down, there is no 
        correlation. We measure correlation on a scale between -1 and 1, 1 being a perfect 
        positive correlation, meaning, as the value of one variable increases, we know that 
        the value of the other variable will go up to, and from the change in the first 
        variable, we could calculate exactly how much the value of the second variable will 
        change. The same is true in reverse for a correlation of -1. For a correlation of 0, 
        we cannot draw any relevant predictions or inverences about one variable from a 
        change in the other.
        <br>
        Given two sets of data points, each describing a seperate variable, Linear Regression<br>
        attempts to fit a straitline that fits these points in in some way optimally. <br>
        <br>
        A strait line on a graph in 2 imentions can be written as y = mx + c, where y is the <br>
        target variable that we want to predict, x is the feature vaiable used for prediction, <br>
        m is the slope of the line on the graph (the gradient) and c is the point at which <br>
        the line crosses the y axis on the graph.<br>
        <br>
        As we are trying to minimise an error function, we need to add it to our equation, <br>
        this makes the equation of our line: y = mx + c + e, where e is the value of the error <br>
        function.<br>
        <br>
        In trying to optimise our model, we want to find the values of m and c that minimises <br>
        e.<br>
        <br>
        To calculate e, we need to decide an error function.<br>
        <br>
        Below is a graph showing some data. We have used the Boston dataset from <br>
        sklearn.datasets which can be imported in the same way as the iris dataset was in our <br>
        initial KNN model. We have plotted the average number of roooms and the average house <br>
        price in thousands.
    </p>

    <img src="Images/plottingdata.png" alt="Plotting our data">

    <p>You may be able to see from this graph that there is a positive correlation between <br>
        number of rooms and house price. This means that as the number of rooms increases, <br>
        we expect the average house price to increase.<br>
        <br>
        if I asked you to predict the price of a house given only the information that it <br>
        has 6 rooms, using this graph, you migh be inclined to choose around the 20000 mark, <br>
        because that is where the data points seem to be clustered. This would be a fair <br>
        guess but we have a better way to make predictions, without doing any calculations <br>
        ourselves.<br>
        <br>
        As mentioned before, Linear Regression tries to plot a line that is in some way <br>
        optimal which it will use for predictions.<br>
        <br>
        Lets assume we have a line that is fit to the data as below.
    </p>

    <img src="Images/firstline.png" alt="Plotting a Line">

    <p>What the Regression model would do is to look for the data given on the x-axis, in our <br>
        case 6 rooms, find the point on the line at which the x-axis equals 6 and then search <br>
        for the house value at that point. In this case around 27,000.
    </p>

    <img src="Images/calculateprediction.png" alt="Calculating a Prediction">

    <p>But as you can see, 25,000 is quite far off what we choose, so were we wrong?<br>
        In the graph above, I drew a line on the graph, but this line wasn't fitted by our<br>
        Linear Regression Model. We can now see where the error function comes in.<br>
        <br>
        But what error function can we use?<br>
        <br>
        Intuitively, we want the line to be as close as possible to all data points whilst <br>
        remaining strait (as the regression is linear).<br>
        <br>
        We could therefore measure the verticaldistance between the line and each data point <br>
        (this is called a residual) and adding them all together. This would give us a value <br>
        known as the sum of the residuals and the start of this calculation is visualised <br>
        below.
    </p>

    <img src="Images/summingresiduals.png" alt="adding residuals">

    <p>The issue with this however is that if we can a positive residual (from a point above <br>
        the line) and a negative residual (from a point below the line), the two residuals <br>
        could cancel each other out! This would mean that there would be number of lines that <br>
        minimise our error function that are very poor fits to the data.<br>
        <br>
        This problem can be solved in two ways:<br>
        <br>
        1. using the absolute sum of residuals (here <br>
        absolute means the positive version of a value, so the absolute value of 3 is 3, and <br>
        the absolute value of -3 is also 3).<br>
        <br>
        2. Using the sum of the squared residuals, taking the square of each residual and <br>
        summing the results. When we minimise this ressult it is called Ordinary Least 
        Squares, or OLS for short.
        <br>
        Here we will use the OLS as our error function. This is the most common choice for <br>
        many reasons, one of which being that it will always give us 1 unique result, whereas <br>
        the sum of the absolute residuals can give us many optimal lines of best fit and <br>
        therefore many 'correct' predictions for the same input.<br>
    </p>

    <h2>Putting it into practice</h2>
    <p>Now that we understand Linear Regression with one predictor variable and one target <br>
        variable, we'll build a Regressor.<br>
        <br>
        As in KNN, we will import a dataset from sci-kit-learn. We will use the Boston <br>
        housing dataset. Here we will try to use the number or rooms, which using the .DESCR <br>
        attribute we can see is denoted as RM, to predict the average house price.<br>
        <br>
        This example will assume you have a basic understanding of numpy and pandas.<br>
        <br>
        First we load the dataset, extract our number of rooms column and average house <br>
        price data into variables and split our data into training and testing data. <br>
        <br>
        In the same way as with KNN, we load the dataset, extract our number of rooms  <br>
        column and average house price data into variables, split our data into training  <br>
        and testing sets, import LogisticRegression, fit our data and then predict on the test set.
    </p>

    <img src="Images/fullworkthrough.png" alt="Instantiate, fit and predict LogisticRegression Model">
    
    <p>Up until now, we have used only 1 predictor variable. This has been to gain an <br>
        understanding of Linear Regression and to allow us to visualise it. It has no problem <br>
        however performing a Regression task with multiple predictor variables. This is known <br>
        as multiple Linear Regression, but is called in almost exactly the same way, below I <br>
        show a Linear Regression model from start to finnish, using the same dataset, but <br>
        with all values used as predictor variables. The three differences to note are, we do <br>
        not need to reshape the feature data as it is not 1 dimentional, we cannot easily <br>
        visualise the data as there are more than 3 dimentions and the predictions have changed.
    </p>

    <img src="Images/multipredcode.png" alt="Multiple Linear Regression">
    <img src="Images/firstthreepreds.png" alt="2 Prediction Results">
    <img src="Images/last2preds.png" alt="3 Prediction Results">

    <p>The maths behiend multiple linear regression is similar to linear regression. Instead of <br>
        using the equation y = mx + c + e, we use an equation y = m1x1 + m2 x2 + ... + mnxn + c + e<br>
        where we have n features, here, we can change mi for any value i and c to fit the best <br>
        line and the error function is calculated in a similar way.
    </p>
    
    <h2>When to use Linear Regression</h2>
    <p>Simple Linear Regression (Linear Regression with only one predictor variable and one 
        target variable) is only an appropriate model to use when we either know, or have 
        reason to believe that their is a correlation, positive or negative between the 
        predictor variable and the target variable, that is to say, if we were to plot a 
        graph with the predictor variable on the x-axis and the target variable on the 
        y-axis, we could draw a strait line on the graph such that all data points would sit 
        on the line, with the only reason any of the data points are offset from the line
        being reasonably described by random noise. This noise must be random, as if the 
        noise also follows a pattern, it can be predicted using a different model.
        
        Multiple Linear Regression is used in much the same circumstances, except that we 
        have more than one predictor variable. The difference being that it is much harder 
        to graph. For Linear Regression to be the appropriate model however, it should be 
        possible to draw a graph for each individual predictor variable in the same ways 
        as above as a Simple Linear Regression Model. If this cannot be done for any of 
        our predictor variable, is it not appropriate to use this variable in our Linear 
        Regression Model.
    </p>
        
    <p>In the next lesson we will see how we can measure the performance of our Regression <br>
        model.
    </p>

    <a href="accuracy.html">Previous: Accuracy Score</a>
    <a href="scoringLinearRegression.html">Up Next: Measuring Performance of Linear Regression</a>

</body>
